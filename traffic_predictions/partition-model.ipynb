{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioned sensors model\n",
    "This notebook trains multiple sensors models for each traffic network partition and writes to HDFS predictions and corresponding real sensor values as well as training and prediction time and finally validation error and training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>629</td><td>application_1533548649762_0075</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1533548649762_0075/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop8:8042/node/containerlogs/container_e57_1533548649762_0075_01_000002/traffic_reginbald__jriv0000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hops import hdfs\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1.8.0'"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "year = 2016\n",
    "month = 11\n",
    "partition_min = 3\n",
    "direction = \"backward\"\n",
    "\n",
    "root_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/\"\n",
    "partition_file_path = root_path + \"partitions/\" + direction + \"_partitions-\" + str(partition_min) + \"min.csv\"\n",
    "sensor_file_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensors-timeseries-parquet/*.parquet\"\n",
    "folder_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-supervised-parquet/\"\n",
    "export_path = root_path + str(year) + \"-\" + str(month) + \"_\" + direction + \"_partitions-\" + str(partition_min) + \"-min-output/\"\n",
    "    \n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "dataset_split = 0.70\n",
    "max_density = 200\n",
    "\n",
    "# Network Parameters\n",
    "past_steps = 10\n",
    "future_steps = 30\n",
    "lstm_units = 200\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "display_step = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = spark.read.parquet(sensor_file_path).columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sensors: 1941\n",
      "First sensor: E182N-0005-1"
     ]
    }
   ],
   "source": [
    "print(\"Number of sensors: \" + str(len(sensors)))\n",
    "print(\"First sensor: \" + sensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_schema = StructType() \\\n",
    "    .add('node', StringType(), False) \\\n",
    "    .add('partition', IntegerType(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_raw_df = spark.read.csv(\n",
    "    partition_file_path, \n",
    "    sep=';', \n",
    "    schema=partition_schema,\n",
    "    ignoreLeadingWhiteSpace=True,\n",
    "    ignoreTrailingWhiteSpace=True,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy/MM/dd HH:mm:ss.SSS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941"
     ]
    }
   ],
   "source": [
    "@udf(StringType())\n",
    "def shot_identifier(identifier):\n",
    "    return identifier[:-2]\n",
    "\n",
    "sensors_df = spark.createDataFrame(sc.parallelize([Row(identifier=s) for s in sensors]), [\"identifier\"]) \\\n",
    "    .withColumn(\"identifier_alt\", shot_identifier(\"identifier\"))\n",
    "sensors_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1941"
     ]
    }
   ],
   "source": [
    "partitions_df = partitions_raw_df.alias(\"p\").join(\n",
    "    sensors_df.alias(\"s\"),\n",
    "    col(\"s.identifier_alt\") == col(\"p.node\"),\n",
    "    \"rightouter\"\n",
    ")\n",
    "partitions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------------+--------------+\n",
      "|node|partition|  identifier|identifier_alt|\n",
      "+----+---------+------------+--------------+\n",
      "|null|     null|E18W-37625-1|    E18W-37625|\n",
      "|null|     null|E18W-37625-2|    E18W-37625|\n",
      "|null|     null|E4_A-31975-1|    E4_A-31975|\n",
      "+----+---------+------------+--------------+"
     ]
    }
   ],
   "source": [
    "# These sensors are not connected to the rest of the graph and should be removed\n",
    "partitions_df.where(col('p.node').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1938"
     ]
    }
   ],
   "source": [
    "partitions_df = partitions_df.where(~col('p.node').isNull()) \\\n",
    "    .select(col(\"s.identifier\").alias(\"identifier\"), col(\"p.partition\").alias(\"partition\"))\n",
    "partitions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_partition_id = partitions_df.agg(max('partition')).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sensors):\n",
    "    columns = [\"t-9\", \"t-8\", \"t-7\", \"t-6\", \"t-5\", \"t-4\", \"t-3\", \"t-2\", \"t-1\", \"t\", \n",
    "               \"t+1\", \"t+2\", \"t+3\", \"t+4\", \"t+5\", \"t+6\", \"t+7\", \"t+8\", \"t+9\", \"t+10\", \n",
    "               \"t+11\", \"t+12\", \"t+13\", \"t+14\", \"t+15\", \"t+16\", \"t+17\", \"t+18\", \"t+19\", \"t+20\", \n",
    "               \"t+21\", \"t+22\", \"t+23\", \"t+24\", \"t+25\", \"t+26\", \"t+27\", \"t+28\", \"t+29\", \"t+30\"]\n",
    "    \n",
    "    data = np.array(np.array(spark.read.parquet(folder_path + sensors[0]).orderBy('Timestamp').select(columns).collect()))\n",
    "    shape = data.shape\n",
    "    data = data.reshape((shape[0], shape[1], 1))\n",
    "    \n",
    "    for i in range(1, len(sensors)):\n",
    "        data = np.append(data, np.array(spark.read.parquet(folder_path + sensors[i]).orderBy('Timestamp').select(columns).collect()).reshape((shape[0], shape[1], 1)), 2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    std = (data - 0) / (max_density - 0)\n",
    "    out = std * (scale_max - scale_min) + scale_min\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, n_sensors):\n",
    "    data_normalized = normalize(data)\n",
    "    x_dataset = data_normalized[:,:10,:]\n",
    "    y_dataset = data_normalized[:,10:,:]\n",
    "    \n",
    "    x_dataset = np.reshape(x_dataset, (-1, past_steps, n_sensors))\n",
    "    y_dataset = np.reshape(y_dataset, (-1, future_steps * n_sensors))\n",
    "    \n",
    "    train_size = int(len(x_dataset) * dataset_split)\n",
    "\n",
    "    x_train = x_dataset[:train_size, :]\n",
    "    x_test = x_dataset[train_size:, :]\n",
    "\n",
    "    y_train = y_dataset[:train_size, :]\n",
    "    y_test = y_dataset[train_size:, :]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(n_sensors):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=lstm_units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=lstm_units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    # Define early stopping criteria\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', # Quantity to be monitored.\n",
    "        min_delta=0.0001,   # Minimum change to qualify as an improvement.\n",
    "        patience=10,         # Number of epochs with no improvement to stop training.\n",
    "        verbose=0,          # Silent\n",
    "        mode='auto'         # Direction of improvement is inferred.\n",
    "    )\n",
    "\n",
    "    # Start time\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model_info = model.fit(\n",
    "        x=x_train,             # Training data\n",
    "        y=y_train,             # Label data\n",
    "        batch_size=batch_size,         # Number of samples per gradient update\n",
    "        epochs=num_epochs,             # Number of iterations over the entire dataset\n",
    "        verbose=0,             # Silent\n",
    "        callbacks=[earlystop], # List of callbacks to apply during training\n",
    "        validation_split=0.2   # Fraction of the training data to be used as validation data\n",
    "    )\n",
    "\n",
    "    # End time\n",
    "    t_end = time.time()\n",
    "    \n",
    "    loss = [float(x) for x in model_info.history['loss']]\n",
    "    val_loss = [float(x) for x in model_info.history['val_loss']]\n",
    "    training_time = (t_end - t_start)\n",
    "    \n",
    "    return loss, val_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(data_scaled):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    std = data_scaled / ((scale_max - scale_min) + scale_min)\n",
    "    data = (std * (max_density - 0)) + 0 \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test):\n",
    "    p_start = time.time()\n",
    "    predictions = model.predict(x_test)\n",
    "    p_end = time.time()\n",
    "    \n",
    "    prediction_time = (p_end - p_start)\n",
    "    \n",
    "    return predictions, prediction_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdfs_file(file_path):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    fd = fs_handle.open_file(file_path, mode='r')\n",
    "    temp_file.write(fd.read())\n",
    "    temp_file.seek(0)\n",
    "    np_array = np.load(temp_file)\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdfs_file(file_path, data):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, data, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(file_path, mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(partition_id, sensors, true_values, pred_values, loss, val_loss, training_time, prediction_time):\n",
    "    \n",
    "    # Export partition specific data\n",
    "    write_hdfs_file(export_path +\"partition_\"+ str(partition_id) + \"/prediction_time.npy\", prediction_time)\n",
    "    write_hdfs_file(export_path +\"partition_\"+ str(partition_id) + \"/training_time.npy\", training_time)\n",
    "    write_hdfs_file(export_path +\"partition_\"+ str(partition_id) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(export_path +\"partition_\"+ str(partition_id) + \"/loss.npy\", loss)\n",
    "    \n",
    "    # Export data for each sensor\n",
    "    for i in range(0, len(sensors)):\n",
    "        write_hdfs_file(export_path + sensors[i] + \"/true_values.npy\", true_values[:,:,i])\n",
    "        write_hdfs_file(export_path + sensors[i] + \"/pred_values.npy\", pred_values[:,:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Evaluation on all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partition_id in range(max_partition_id + 1):\n",
    "    partition_sensors = partitions_df.where(col(\"partition\") == partition_id).rdd.map(lambda row: row[\"identifier\"]).collect()\n",
    "    data = load_data(partition_sensors)\n",
    "    x_train, y_train, x_test, y_test = prepare_dataset(data, len(partition_sensors))\n",
    "    model = define_model(len(partition_sensors))\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    predictions, prediction_time = evaluate_model(model, x_test)\n",
    "    y_values = np.reshape(np.array([denormalize(y) for y in y_test]), (-1, future_steps, len(partition_sensors)))\n",
    "    pred_values = np.reshape(np.array([denormalize(y) for y in predictions]), (-1, future_steps, len(partition_sensors)))\n",
    "    export_data(partition_id, partition_sensors, y_values, pred_values, loss, val_loss, training_time, prediction_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(9.78538299)"
     ]
    }
   ],
   "source": [
    "read_hdfs_file(export_path +\"partition_\"+ str(max_partition_id) + \"/prediction_time.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(429.19977212)"
     ]
    }
   ],
   "source": [
    "read_hdfs_file(export_path +\"partition_\"+ str(max_partition_id) + \"/training_time.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.00071579, 0.00066156, 0.00070745, 0.00064122, 0.00063627,\n",
      "       0.0006209 , 0.00064779, 0.00064366, 0.00065276, 0.00063779,\n",
      "       0.00064346])"
     ]
    }
   ],
   "source": [
    "read_hdfs_file(export_path +\"partition_\"+ str(max_partition_id) + \"/val_loss.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.00103688, 0.00070439, 0.00065093, 0.0006119 , 0.00058769,\n",
      "       0.00056787, 0.00055059, 0.00053865, 0.00052679, 0.00051777,\n",
      "       0.00050831])"
     ]
    }
   ],
   "source": [
    "read_hdfs_file(export_path +\"partition_\"+ str(max_partition_id) + \"/loss.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_true = read_hdfs_file(export_path + \"E4N-47465-1/true_values.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_true = read_hdfs_file(root_path + \"2016-11_single-sensor-30-min-output/E4N-47465-1/true_values.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_true = np.reshape(single_true, partition_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False"
     ]
    }
   ],
   "source": [
    "(single_true == partition_true).sum() == 352628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12085, 30)"
     ]
    }
   ],
   "source": [
    "partition_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

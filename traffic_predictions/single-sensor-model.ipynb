{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single sensor model\n",
    "This notebook trains single sensor models for all Trafikverket sensors and writes to HDFS predictions and corresponding real sensor values as well as training and prediction time and finally validation error and training error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1231</td><td>application_1536227070932_0750</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1536227070932_0750/\">Link</a></td><td><a target=\"_blank\" href=\"http://gpu1:8042/node/containerlogs/container_e59_1536227070932_0750_01_000001/traffic_reginbald__jriv0000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hops import hdfs\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1.8.0'"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "year = 2016\n",
    "month = 11\n",
    "\n",
    "root_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/\"\n",
    "sensor_file_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensors-timeseries-parquet/*.parquet\"\n",
    "folder_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-supervised-parquet/\"\n",
    "export_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-output/\"\n",
    "export_gru_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-gru-30-min-output/\"\n",
    "    \n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "dataset_split = 0.70\n",
    "max_density = 200\n",
    "\n",
    "# Network Parameters\n",
    "past_steps = 10\n",
    "future_steps = 30\n",
    "n_sensors = 1\n",
    "lstm_units = 50\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "display_step = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = spark.read.parquet(sensor_file_path).columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sensors: \" + str(len(sensors)))\n",
    "print(\"First sensor: \" + sensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, columns):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for column in columns:\n",
    "        std = (data[column] - 0) / (max_density - 0)\n",
    "        out.append( std * (scale_max - scale_min) + scale_min)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    columns = df.columns[1:]\n",
    "    df_normalized = df.rdd.map(lambda row: normalize(row, columns))\n",
    "    x_dataset = np.array(df_normalized.map(lambda row: row[:10]).collect())\n",
    "    y_dataset = np.array(df_normalized.map(lambda row: row[10:]).collect())\n",
    "    \n",
    "    x_dataset = np.reshape(x_dataset, (-1, past_steps, n_sensors))\n",
    "    y_dataset = np.reshape(y_dataset, (-1, future_steps * n_sensors))\n",
    "    \n",
    "    train_size = int(len(x_dataset) * dataset_split)\n",
    "\n",
    "    x_train = x_dataset[:train_size, :]\n",
    "    x_test = x_dataset[train_size:, :]\n",
    "\n",
    "    y_train = y_dataset[:train_size, :]\n",
    "    y_test = y_dataset[train_size:, :]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=lstm_units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=lstm_units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gru_model():\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    gru_1 = tf.keras.layers.LSTM(units=50, return_sequences=True)(inputs)\n",
    "    gru_2 = tf.keras.layers.LSTM(units=50)(gru_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(gru_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    # Define early stopping criteria\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', # Quantity to be monitored.\n",
    "        min_delta=0.0001,   # Minimum change to qualify as an improvement.\n",
    "        patience=10,         # Number of epochs with no improvement to stop training.\n",
    "        verbose=0,          # Silent\n",
    "        mode='auto'         # Direction of improvement is inferred.\n",
    "    )\n",
    "\n",
    "    # Start time\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model_info = model.fit(\n",
    "        x=x_train,             # Training data\n",
    "        y=y_train,             # Label data\n",
    "        batch_size=batch_size,         # Number of samples per gradient update\n",
    "        epochs=num_epochs,             # Number of iterations over the entire dataset\n",
    "        verbose=0,             # Silent\n",
    "        callbacks=[earlystop], # List of callbacks to apply during training\n",
    "        validation_split=0.2   # Fraction of the training data to be used as validation data\n",
    "    )\n",
    "\n",
    "    # End time\n",
    "    t_end = time.time()\n",
    "    \n",
    "    loss = [float(x) for x in model_info.history['loss']]\n",
    "    val_loss = [float(x) for x in model_info.history['val_loss']]\n",
    "    training_time = (t_end - t_start)\n",
    "    \n",
    "    return loss, val_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(data_scaled):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    std = data_scaled / ((scale_max - scale_min) + scale_min)\n",
    "    data = (std * (max_density - 0)) + 0 \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, x_test):\n",
    "    p_start = time.time()\n",
    "    predictions = model.predict(x_test)\n",
    "    p_end = time.time()\n",
    "    \n",
    "    prediction_time = (p_end - p_start)\n",
    "    \n",
    "    return predictions, prediction_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(sensor, y_values, pred_values, loss, val_loss, training_time, prediction_time, path):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    \n",
    "    # Write actual sensor values\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, y_values, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/true_values.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()\n",
    "    \n",
    "    # Write predicted sensor values\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, pred_values, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/pred_values.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()\n",
    "    \n",
    "    # Write training losses\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, loss, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/loss.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()\n",
    "    \n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, val_loss, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/val_loss.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()\n",
    "    \n",
    "    # Write training_time, prediction_time\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, training_time, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/training_time.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()\n",
    "    \n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, prediction_time, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(path + sensor + \"/prediction_time.npy\", mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Evaluation on all sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1091"
     ]
    }
   ],
   "source": [
    "len(sensors[850:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensors:\n",
    "    sensor_df = spark.read.parquet(folder_path + sensor).orderBy('Timestamp')\n",
    "    x_train, y_train, x_test, y_test = prepare_dataset(sensor_df)\n",
    "    model = define_model()\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    predictions, prediction_time = evaluate_model(model, x_test)\n",
    "    y_values = np.reshape(np.array([denormalize(y) for y in y_test]), (-1, future_steps, n_sensors))\n",
    "    pred_values = np.reshape(np.array([denormalize(y) for y in predictions]), (-1, future_steps, n_sensors))\n",
    "    export_data(sensor, y_values, pred_values, loss, val_loss, training_time, prediction_time, export_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sensor in sensors[850:]:\n",
    "    sensor_df = spark.read.parquet(folder_path + sensor).orderBy('Timestamp')\n",
    "    x_train, y_train, x_test, y_test = prepare_dataset(sensor_df)\n",
    "    model = define_gru_model()\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    predictions, prediction_time = evaluate_model(model, x_test)\n",
    "    y_values = np.reshape(np.array([denormalize(y) for y in y_test]), (-1, future_steps, n_sensors))\n",
    "    pred_values = np.reshape(np.array([denormalize(y) for y in predictions]), (-1, future_steps, n_sensors))\n",
    "    export_data(sensor, y_values, pred_values, loss, val_loss, training_time, prediction_time, export_gru_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[16.27118644],\n",
      "        [13.5483871 ],\n",
      "        [ 7.38461538],\n",
      "        ...,\n",
      "        [11.80327869],\n",
      "        [14.75409836],\n",
      "        [ 7.27272727]],\n",
      "\n",
      "       [[13.5483871 ],\n",
      "        [ 7.38461538],\n",
      "        [ 9.56521739],\n",
      "        ...,\n",
      "        [14.75409836],\n",
      "        [ 7.27272727],\n",
      "        [ 7.5       ]],\n",
      "\n",
      "       [[ 7.38461538],\n",
      "        [ 9.56521739],\n",
      "        [ 9.        ],\n",
      "        ...,\n",
      "        [ 7.27272727],\n",
      "        [ 7.5       ],\n",
      "        [19.65517241]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 0.77922078],\n",
      "        [ 0.83333333],\n",
      "        [ 1.6       ],\n",
      "        ...,\n",
      "        [ 0.90909091],\n",
      "        [ 0.        ],\n",
      "        [ 0.89552239]],\n",
      "\n",
      "       [[ 0.83333333],\n",
      "        [ 1.6       ],\n",
      "        [ 0.84507042],\n",
      "        ...,\n",
      "        [ 0.        ],\n",
      "        [ 0.89552239],\n",
      "        [ 0.        ]],\n",
      "\n",
      "       [[ 1.6       ],\n",
      "        [ 0.84507042],\n",
      "        [ 0.        ],\n",
      "        ...,\n",
      "        [ 0.89552239],\n",
      "        [ 0.        ],\n",
      "        [ 0.        ]]])"
     ]
    }
   ],
   "source": [
    "# Read true values from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/true_values.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[[18.242292 ],\n",
      "        [16.053202 ],\n",
      "        [16.598661 ],\n",
      "        ...,\n",
      "        [16.533257 ],\n",
      "        [15.520663 ],\n",
      "        [15.887652 ]],\n",
      "\n",
      "       [[15.959862 ],\n",
      "        [15.015607 ],\n",
      "        [15.699027 ],\n",
      "        ...,\n",
      "        [16.213816 ],\n",
      "        [15.175654 ],\n",
      "        [15.574648 ]],\n",
      "\n",
      "       [[10.081504 ],\n",
      "        [11.78884  ],\n",
      "        [12.721829 ],\n",
      "        ...,\n",
      "        [14.524057 ],\n",
      "        [13.390411 ],\n",
      "        [13.874653 ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 1.4464101],\n",
      "        [ 1.1097412],\n",
      "        [ 1.3072675],\n",
      "        ...,\n",
      "        [ 2.6973233],\n",
      "        [ 1.0778595],\n",
      "        [ 1.7493668]],\n",
      "\n",
      "       [[ 1.2951547],\n",
      "        [ 1.0152605],\n",
      "        [ 1.2178056],\n",
      "        ...,\n",
      "        [ 2.6383572],\n",
      "        [ 1.0159919],\n",
      "        [ 1.6899004]],\n",
      "\n",
      "       [[ 2.0018685],\n",
      "        [ 1.4056709],\n",
      "        [ 1.5765277],\n",
      "        ...,\n",
      "        [ 2.8382058],\n",
      "        [ 1.2251537],\n",
      "        [ 1.8891755]]], dtype=float32)"
     ]
    }
   ],
   "source": [
    "# Read predicted values from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/pred_values.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(24.96166301)"
     ]
    }
   ],
   "source": [
    "# Read prediction time from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/prediction_time.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(135.72623706)"
     ]
    }
   ],
   "source": [
    "# Read prediction time from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/training_time.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.00096028, 0.00090014, 0.00102293, 0.00093372, 0.00096108,\n",
      "       0.00089569, 0.00095728, 0.00092306, 0.00089398, 0.00087433,\n",
      "       0.00092681])"
     ]
    }
   ],
   "source": [
    "# Read validation loss from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/val_loss.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0.00246245, 0.00203831, 0.00197797, 0.00194428, 0.00190776,\n",
      "       0.00189846, 0.00190155, 0.00190121, 0.00190021, 0.00189787,\n",
      "       0.00187787])"
     ]
    }
   ],
   "source": [
    "# Read loss from HDFS\n",
    "\n",
    "fs_handle = hdfs.get_fs()\n",
    "temp_file = TemporaryFile()\n",
    "\n",
    "fd = fs_handle.open_file(export_path + sensors[-1] + \"/loss.npy\", mode='r')\n",
    "\n",
    "temp_file.write(fd.read())\n",
    "temp_file.seek(0) # important, set cursor to beginning of file\n",
    "\n",
    "np_array = np.load(temp_file)\n",
    "np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning - Partition model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hops import hdfs\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "year = 2016\n",
    "month = 11\n",
    "partition_min = 3\n",
    "partition_id_to_tune = 30\n",
    "direction = \"backward\"\n",
    "\n",
    "root_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/\"\n",
    "partition_file_path = root_path + \"partitions/\" + direction + \"_partitions-\" + str(partition_min) + \"min.csv\"\n",
    "sensor_file_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensors-timeseries-parquet/*.parquet\"\n",
    "data_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-supervised-parquet/\"\n",
    "    \n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "dataset_split = 0.70\n",
    "max_density = 200\n",
    "\n",
    "# Network Parameters\n",
    "past_steps = 10\n",
    "future_steps = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdfs_file(file_path):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    fd = fs_handle.open_file(file_path, mode='r')\n",
    "    temp_file.write(fd.read())\n",
    "    temp_file.seek(0)\n",
    "    np_array = np.load(temp_file)\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdfs_file(file_path, data):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, data, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(file_path, mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = spark.read.parquet(sensor_file_path).columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of sensors: \" + str(len(sensors)))\n",
    "print(\"First sensor: \" + sensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_schema = StructType() \\\n",
    "    .add('node', StringType(), False) \\\n",
    "    .add('partition', IntegerType(), False) \\\n",
    "    .add('group', StringType(), False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_raw_df = spark.read.csv(\n",
    "    partition_file_path, \n",
    "    sep=';', \n",
    "    schema=partition_schema,\n",
    "    ignoreLeadingWhiteSpace=True,\n",
    "    ignoreTrailingWhiteSpace=True,\n",
    "    header=True,\n",
    "    timestampFormat='yyyy/MM/dd HH:mm:ss.SSS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(StringType())\n",
    "def shot_identifier(identifier):\n",
    "    return identifier[:-2]\n",
    "\n",
    "sensors_df = spark.createDataFrame(sc.parallelize([Row(identifier=s) for s in sensors]), [\"identifier\"]) \\\n",
    "    .withColumn(\"identifier_alt\", shot_identifier(\"identifier\"))\n",
    "sensors_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_df = partitions_raw_df.alias(\"p\").join(\n",
    "    sensors_df.alias(\"s\"),\n",
    "    col(\"s.identifier_alt\") == col(\"p.node\"),\n",
    "    \"rightouter\"\n",
    ")\n",
    "partitions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These sensors are not connected to the rest of the graph and should be removed\n",
    "partitions_df.where(col('p.node').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_df = partitions_df.where(~col('p.node').isNull()) \\\n",
    "    .select(col(\"s.identifier\").alias(\"identifier\"), col(\"p.partition\").alias(\"partition\"))\n",
    "partitions_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_partition_id = partitions_df.agg(max('partition')).collect()[0][0]\n",
    "max_partition_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitions_df.groupBy('partition').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(sensors):\n",
    "    columns = [\"t-9\", \"t-8\", \"t-7\", \"t-6\", \"t-5\", \"t-4\", \"t-3\", \"t-2\", \"t-1\", \"t\", \n",
    "               \"t+1\", \"t+2\", \"t+3\", \"t+4\", \"t+5\", \"t+6\", \"t+7\", \"t+8\", \"t+9\", \"t+10\", \n",
    "               \"t+11\", \"t+12\", \"t+13\", \"t+14\", \"t+15\", \"t+16\", \"t+17\", \"t+18\", \"t+19\", \"t+20\", \n",
    "               \"t+21\", \"t+22\", \"t+23\", \"t+24\", \"t+25\", \"t+26\", \"t+27\", \"t+28\", \"t+29\", \"t+30\"]\n",
    "    \n",
    "    data = np.array(np.array(spark.read.parquet(data_path + sensors[0]).orderBy('Timestamp').select(columns).collect()))\n",
    "    shape = data.shape\n",
    "    data = data.reshape((shape[0], shape[1], 1))\n",
    "    \n",
    "    for i in range(1, len(sensors)):\n",
    "        data = np.append(data, np.array(spark.read.parquet(data_path + sensors[i]).orderBy('Timestamp').select(columns).collect()).reshape((shape[0], shape[1], 1)), 2)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    std = (data - 0) / (max_density - 0)\n",
    "    out = std * (scale_max - scale_min) + scale_min\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(data, n_sensors):\n",
    "    data_normalized = normalize(data)\n",
    "    x_dataset = data_normalized[:,:10,:]\n",
    "    y_dataset = data_normalized[:,10:,:]\n",
    "    \n",
    "    x_dataset = np.reshape(x_dataset, (-1, past_steps, n_sensors))\n",
    "    y_dataset = np.reshape(y_dataset, (-1, future_steps * n_sensors))\n",
    "    \n",
    "    train_size = int(len(x_dataset) * dataset_split)\n",
    "\n",
    "    x_train = x_dataset[:train_size, :]\n",
    "    x_test = x_dataset[train_size:, :]\n",
    "\n",
    "    y_train = y_dataset[:train_size, :]\n",
    "    y_test = y_dataset[train_size:, :]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(l_units, n_sensors):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=l_units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=l_units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    # Define early stopping criteria\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', # Quantity to be monitored.\n",
    "        min_delta=0.0001,   # Minimum change to qualify as an improvement.\n",
    "        patience=10,         # Number of epochs with no improvement to stop training.\n",
    "        verbose=0,          # Silent\n",
    "        mode='auto'         # Direction of improvement is inferred.\n",
    "    )\n",
    "\n",
    "    # Start time\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model_info = model.fit(\n",
    "        x=x_train,             # Training data\n",
    "        y=y_train,             # Label data\n",
    "        batch_size=batch_size,         # Number of samples per gradient update\n",
    "        epochs=num_epochs,             # Number of iterations over the entire dataset\n",
    "        verbose=0,             # Silent\n",
    "        callbacks=[earlystop], # List of callbacks to apply during training\n",
    "        validation_split=0.2   # Fraction of the training data to be used as validation data\n",
    "    )\n",
    "\n",
    "    # End time\n",
    "    t_end = time.time()\n",
    "    \n",
    "    loss = [float(x) for x in model_info.history['loss']]\n",
    "    val_loss = [float(x) for x in model_info.history['val_loss']]\n",
    "    training_time = (t_end - t_start)\n",
    "    \n",
    "    return loss, val_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training and Evaluation on all partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_sensors = partitions_df.where(col(\"partition\") == partition_id_to_tune).rdd.map(lambda row: row[\"identifier\"]).collect()\n",
    "data = load_data(partition_sensors)\n",
    "x_train, y_train, x_test, y_test = prepare_dataset(data, len(partition_sensors))\n",
    "\n",
    "\n",
    "for lstm_units in [100]: #[200, 400, 600, 800, 1000]:\n",
    "    model = define_model(lstm_units, len(partition_sensors))\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/partition_\" + str(partition_min) + \\\n",
    "                    \"min-lstm_units_\" + str(lstm_units) + \"/loss.npy\", loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/partition_\" + str(partition_min) + \\\n",
    "                    \"min-lstm_units_\" + str(lstm_units) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/partition_\" + str(partition_min) + \\\n",
    "                    \"min-lstm_units_\" + str(lstm_units) + \"/training_time.npy\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning - single sensor model\n",
    "This notebook does a hyperparameter search for the single sensor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1506</td><td>application_1544690131655_0130</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop33:8088/proxy/application_1544690131655_0130/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop11:8042/node/containerlogs/container_e68_1544690131655_0130_01_000001/traffic_reginbald__jriv0000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hops import hdfs, util\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1.8.0'"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "year = 2016\n",
    "month = 11\n",
    "\n",
    "root_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/\"\n",
    "sensor_file_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensors-timeseries-parquet/*.parquet\"\n",
    "folder_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-supervised-parquet/\"\n",
    "export_path = root_path + str(year) + \"-\" + str(month) + \"_single-sensor-30-min-output/\"\n",
    "    \n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "dataset_split = 0.70\n",
    "max_density = 200\n",
    "\n",
    "# Network Parameters\n",
    "past_steps = 10\n",
    "future_steps = 30\n",
    "n_sensors = 1\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "display_step = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdfs_file(file_path, data):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, data, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(file_path, mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = spark.read.parquet(sensor_file_path).columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sensors: 1941\n",
      "First sensor: E182N-0005-1"
     ]
    }
   ],
   "source": [
    "print(\"Number of sensors: \" + str(len(sensors)))\n",
    "print(\"First sensor: \" + sensors[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, columns):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for column in columns:\n",
    "        std = (data[column] - 0) / (max_density - 0)\n",
    "        out.append( std * (scale_max - scale_min) + scale_min)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df):\n",
    "    columns = df.columns[1:]\n",
    "    df_normalized = df.rdd.map(lambda row: normalize(row, columns))\n",
    "    x_dataset = np.array(df_normalized.map(lambda row: row[:10]).collect())\n",
    "    y_dataset = np.array(df_normalized.map(lambda row: row[10:]).collect())\n",
    "    \n",
    "    x_dataset = np.reshape(x_dataset, (-1, past_steps, n_sensors))\n",
    "    y_dataset = np.reshape(y_dataset, (-1, future_steps * n_sensors))\n",
    "    \n",
    "    train_size = int(len(x_dataset) * dataset_split)\n",
    "\n",
    "    x_train = x_dataset[:train_size, :]\n",
    "    x_test = x_dataset[train_size:, :]\n",
    "\n",
    "    y_train = y_dataset[:train_size, :]\n",
    "    y_test = y_dataset[train_size:, :]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(lstm_units):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=lstm_units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=lstm_units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gru_model(units):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.GRU(units=units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.GRU(units=units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    # Define early stopping criteria\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', # Quantity to be monitored.\n",
    "        min_delta=0.0001,   # Minimum change to qualify as an improvement.\n",
    "        patience=10,         # Number of epochs with no improvement to stop training.\n",
    "        verbose=0,          # Silent\n",
    "        mode='auto'         # Direction of improvement is inferred.\n",
    "    )\n",
    "\n",
    "    # Start time\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model_info = model.fit(\n",
    "        x=x_train,             # Training data\n",
    "        y=y_train,             # Label data\n",
    "        batch_size=batch_size,         # Number of samples per gradient update\n",
    "        epochs=num_epochs,             # Number of iterations over the entire dataset\n",
    "        verbose=2,             # Show progress bar\n",
    "        callbacks=[earlystop], # List of callbacks to apply during training\n",
    "        validation_split=0.2   # Fraction of the training data to be used as validation data\n",
    "    )\n",
    "\n",
    "    # End time\n",
    "    t_end = time.time()\n",
    "    \n",
    "    loss = [float(x) for x in model_info.history['loss']]\n",
    "    val_loss = [float(x) for x in model_info.history['val_loss']]\n",
    "    training_time = (t_end - t_start)\n",
    "    \n",
    "    return loss, val_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22556 samples, validate on 5640 samples\n",
      "Epoch 1/100\n",
      " - 680s - loss: 3.0826e-04 - val_loss: 1.2323e-04\n",
      "Epoch 2/100\n",
      " - 808s - loss: 2.6753e-04 - val_loss: 1.4952e-04\n",
      "Epoch 3/100\n",
      " - 856s - loss: 2.4483e-04 - val_loss: 1.4767e-04\n",
      "Epoch 4/100\n",
      " - 855s - loss: 2.4171e-04 - val_loss: 1.3934e-04\n",
      "Epoch 5/100\n",
      " - 829s - loss: 2.3552e-04 - val_loss: 1.0876e-04\n",
      "Epoch 6/100\n",
      " - 833s - loss: 2.2807e-04 - val_loss: 1.0361e-04\n",
      "Epoch 7/100\n",
      " - 838s - loss: 2.2300e-04 - val_loss: 1.0627e-04\n",
      "Epoch 8/100\n",
      " - 845s - loss: 2.2805e-04 - val_loss: 1.1954e-04\n",
      "Epoch 9/100\n",
      " - 829s - loss: 2.2237e-04 - val_loss: 1.1009e-04\n",
      "Epoch 10/100\n",
      " - 842s - loss: 2.2352e-04 - val_loss: 1.0974e-04\n",
      "Epoch 11/100\n",
      " - 837s - loss: 2.2212e-04 - val_loss: 1.0982e-04"
     ]
    }
   ],
   "source": [
    "sensor_df = spark.read.parquet(folder_path + sensors[0]).orderBy('Timestamp')\n",
    "x_train, y_train, x_test, y_test = prepare_dataset(sensor_df)\n",
    "\n",
    "for lstm_units in [50, 100, 500, 1000]:\n",
    "    model = define_model(lstm_units)\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-lstm_units_\" + \\\n",
    "                    str(lstm_units) + \"/loss.npy\", loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-lstm_units_\" + \\\n",
    "                    str(lstm_units) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-lstm_units_\" + \\\n",
    "                    str(lstm_units) + \"/training_time.npy\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22556 samples, validate on 5640 samples\n",
      "Epoch 1/100\n",
      " - 5s - loss: 2.6877e-04 - val_loss: 1.0693e-04\n",
      "Epoch 2/100\n",
      " - 4s - loss: 2.4564e-04 - val_loss: 1.1004e-04\n",
      "Epoch 3/100\n",
      " - 4s - loss: 2.4114e-04 - val_loss: 1.2057e-04\n",
      "Epoch 4/100\n",
      " - 4s - loss: 2.3447e-04 - val_loss: 1.1731e-04\n",
      "Epoch 5/100\n",
      " - 4s - loss: 2.3522e-04 - val_loss: 1.1054e-04\n",
      "Epoch 6/100\n",
      " - 4s - loss: 2.3644e-04 - val_loss: 1.1921e-04\n",
      "Epoch 7/100\n",
      " - 4s - loss: 2.3413e-04 - val_loss: 1.1009e-04\n",
      "Epoch 8/100\n",
      " - 4s - loss: 2.3465e-04 - val_loss: 1.0538e-04\n",
      "Epoch 9/100\n",
      " - 4s - loss: 2.3162e-04 - val_loss: 1.0568e-04\n",
      "Epoch 10/100\n",
      " - 4s - loss: 2.3184e-04 - val_loss: 1.1066e-04\n",
      "Epoch 11/100\n",
      " - 4s - loss: 2.2575e-04 - val_loss: 1.0637e-04\n",
      "Train on 22556 samples, validate on 5640 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 2.6839e-04 - val_loss: 1.3260e-04\n",
      "Epoch 2/100\n",
      " - 6s - loss: 2.4681e-04 - val_loss: 1.2910e-04\n",
      "Epoch 3/100\n",
      " - 6s - loss: 2.4407e-04 - val_loss: 1.1503e-04\n",
      "Epoch 4/100\n",
      " - 6s - loss: 2.4505e-04 - val_loss: 1.1469e-04\n",
      "Epoch 5/100\n",
      " - 6s - loss: 2.4513e-04 - val_loss: 1.1078e-04\n",
      "Epoch 6/100\n",
      " - 6s - loss: 2.4061e-04 - val_loss: 1.1125e-04\n",
      "Epoch 7/100\n",
      " - 6s - loss: 2.3791e-04 - val_loss: 1.0914e-04\n",
      "Epoch 8/100\n",
      " - 6s - loss: 2.3375e-04 - val_loss: 1.1646e-04\n",
      "Epoch 9/100\n",
      " - 6s - loss: 2.3531e-04 - val_loss: 1.2029e-04\n",
      "Epoch 10/100\n",
      " - 6s - loss: 2.4338e-04 - val_loss: 1.2321e-04\n",
      "Epoch 11/100\n",
      " - 6s - loss: 2.3541e-04 - val_loss: 1.0915e-04\n",
      "Train on 22556 samples, validate on 5640 samples\n",
      "Epoch 1/100\n",
      " - 53s - loss: 3.1190e-04 - val_loss: 1.6256e-04\n",
      "Epoch 2/100\n",
      " - 52s - loss: 2.6177e-04 - val_loss: 1.1989e-04\n",
      "Epoch 3/100\n",
      " - 52s - loss: 2.5181e-04 - val_loss: 1.2756e-04\n",
      "Epoch 4/100\n",
      " - 51s - loss: 2.4412e-04 - val_loss: 1.1862e-04\n",
      "Epoch 5/100\n",
      " - 52s - loss: 2.4659e-04 - val_loss: 1.2612e-04\n",
      "Epoch 6/100\n",
      " - 51s - loss: 2.4687e-04 - val_loss: 1.1077e-04\n",
      "Epoch 7/100\n",
      " - 51s - loss: 2.4651e-04 - val_loss: 1.2153e-04\n",
      "Epoch 8/100\n",
      " - 51s - loss: 2.5591e-04 - val_loss: 1.4806e-04\n",
      "Epoch 9/100\n",
      " - 51s - loss: 2.5286e-04 - val_loss: 1.1555e-04\n",
      "Epoch 10/100\n",
      " - 52s - loss: 2.4502e-04 - val_loss: 1.0625e-04\n",
      "Epoch 11/100\n",
      " - 51s - loss: 2.3304e-04 - val_loss: 1.2267e-04\n",
      "Train on 22556 samples, validate on 5640 samples\n",
      "Epoch 1/100\n",
      " - 190s - loss: 3.1024e-04 - val_loss: 1.2740e-04\n",
      "Epoch 2/100\n",
      " - 187s - loss: 2.7123e-04 - val_loss: 1.6774e-04\n",
      "Epoch 3/100\n",
      " - 187s - loss: 2.6127e-04 - val_loss: 2.4279e-04\n",
      "Epoch 4/100\n",
      " - 189s - loss: 2.5365e-04 - val_loss: 1.2220e-04\n",
      "Epoch 5/100\n",
      " - 189s - loss: 2.3906e-04 - val_loss: 1.3903e-04\n",
      "Epoch 6/100\n",
      " - 191s - loss: 2.9489e-04 - val_loss: 2.2335e-04\n",
      "Epoch 7/100\n",
      " - 191s - loss: 2.8218e-04 - val_loss: 1.1905e-04\n",
      "Epoch 8/100\n",
      " - 191s - loss: 2.4317e-04 - val_loss: 1.3800e-04\n",
      "Epoch 9/100\n",
      " - 190s - loss: 2.3362e-04 - val_loss: 1.1164e-04\n",
      "Epoch 10/100\n",
      " - 191s - loss: 2.3206e-04 - val_loss: 1.2383e-04\n",
      "Epoch 11/100\n",
      " - 191s - loss: 2.2547e-04 - val_loss: 1.2528e-04"
     ]
    }
   ],
   "source": [
    "sensor_df = spark.read.parquet(folder_path + sensors[0]).orderBy('Timestamp')\n",
    "x_train, y_train, x_test, y_test = prepare_dataset(sensor_df)\n",
    "\n",
    "for units in [50, 100, 500, 1000]:\n",
    "    model = define_gru_model(units)\n",
    "    loss, val_loss, training_time = train_model(model, x_train, y_train)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-gru_units_\" + \\\n",
    "                    str(units) + \"/loss.npy\", loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-gru_units_\" + \\\n",
    "                    str(units) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/single_sensor-gru_units_\" + \\\n",
    "                    str(units) + \"/training_time.npy\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

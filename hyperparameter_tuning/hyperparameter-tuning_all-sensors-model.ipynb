{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>719</td><td>application_1535116440643_0118</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop30:8088/proxy/application_1535116440643_0118/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop23:8042/node/containerlogs/container_e58_1535116440643_0118_01_000001/traffic_reginbald__jriv0000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from hops import hdfs\n",
    "from tempfile import TemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1.8.0'"
     ]
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Parameters\n",
    "year = 2016\n",
    "month = 11\n",
    "\n",
    "root_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/\"\n",
    "file_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensors-30-min-supervised-parquet/*.parquet\"\n",
    "index_path = root_path + str(year) + \"-\" + str(month) + \"_sensor-to-index-30-min/*.parquet\"\n",
    "export_path = root_path + str(year) + \"-\" + str(month) + \"_all-sensor-model-output/\"\n",
    "\n",
    "batch_size = 100\n",
    "num_epochs = 100\n",
    "dataset_split = 0.70\n",
    "max_density = 200\n",
    "\n",
    "# Network Parameters\n",
    "past_steps = 10\n",
    "future_steps = 5\n",
    "n_sensors = 1941\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.001\n",
    "display_step = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_hdfs_file(file_path):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    fd = fs_handle.open_file(file_path, mode='r')\n",
    "    temp_file.write(fd.read())\n",
    "    temp_file.seek(0)\n",
    "    np_array = np.load(temp_file)\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_hdfs_file(file_path, data):\n",
    "    fs_handle = hdfs.get_fs()\n",
    "    temp_file = TemporaryFile()\n",
    "    np.save(temp_file, data, allow_pickle=False)\n",
    "    temp_file.seek(0)\n",
    "\n",
    "    fd = fs_handle.open_file(file_path, mode='w')\n",
    "    fd.write(temp_file.read())\n",
    "    fd.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40281"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(file_path).orderBy('Timestamp')\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dict = dict([(row['key'], row['index']) for row in spark.read.parquet(index_path).orderBy('index').collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, columns):\n",
    "    scale_min = 0\n",
    "    scale_max = 1\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    for column in columns:\n",
    "        std = (np.array(data[column]) - 0) / (max_density - 0)\n",
    "        out.append( std * (scale_max - scale_min) + scale_min)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['t-9', 't-8', 't-7', 't-6', 't-5', 't-4', 't-3', 't-2', 't-1', 't', 't+3', 't+5', 't+10', 't+20', 't+30']\n",
    "\n",
    "df_normalized = df.select(columns).rdd.map(lambda row: normalize(row, columns))\n",
    "x_dataset = np.array(df_normalized.map(lambda row: row[:past_steps]).collect())\n",
    "y_dataset = np.array(df_normalized.map(lambda row: row[past_steps:]).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.01285714 0.00759494 0.00333333 ... 0.         0.         0.        ]\n",
      " [0.         0.00428571 0.00952381 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.009      0.0122449  0.0137931  ... 0.         0.         0.        ]\n",
      " [0.00689655 0.00731707 0.004      ... 0.         0.         0.        ]\n",
      " [0.00322581 0.00326087 0.00348837 ... 0.         0.         0.        ]]\n",
      "(40281, 10, 1941)\n",
      "(40281, 5, 1941)"
     ]
    }
   ],
   "source": [
    "print x_dataset[0]\n",
    "print y_dataset[0]\n",
    "\n",
    "print x_dataset.shape\n",
    "print y_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((28196, 10, 1941), (28196, 9705), (12085, 10, 1941), (12085, 9705))"
     ]
    }
   ],
   "source": [
    "x_dataset = np.reshape(x_dataset, (-1, past_steps, n_sensors))\n",
    "y_dataset = np.reshape(y_dataset, (-1, future_steps * n_sensors))\n",
    "\n",
    "train_size = int(len(x_dataset) * dataset_split)\n",
    "\n",
    "x_train = x_dataset[:train_size, :]\n",
    "x_test = x_dataset[train_size:, :]\n",
    "\n",
    "y_train = y_dataset[:train_size, :]\n",
    "y_test = y_dataset[train_size:, :]\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(lstm_units):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    lstm_1 = tf.keras.layers.LSTM(units=lstm_units, return_sequences=True)(inputs)\n",
    "    lstm_2 = tf.keras.layers.LSTM(units=lstm_units)(lstm_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(lstm_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define GRU Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gru_model(gru_units):\n",
    "    inputs = tf.keras.Input(shape=(past_steps, n_sensors))\n",
    "\n",
    "    gru_1 = tf.keras.layers.GRU(units=gru_units, return_sequences=True)(inputs)\n",
    "    gru_2 = tf.keras.layers.GRU(units=gru_units)(gru_1)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(\n",
    "        units=500,\n",
    "        activation='linear',\n",
    "        kernel_constraint=tf.keras.constraints.NonNeg() \n",
    "    )(gru_2)\n",
    "\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        units=future_steps * n_sensors\n",
    "    )(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',         # Optimizer to use.\n",
    "        loss='mean_squared_error' # Loss function to use.\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, x_train, y_train):\n",
    "    # Define early stopping criteria\n",
    "    earlystop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', # Quantity to be monitored.\n",
    "        min_delta=0.0001,   # Minimum change to qualify as an improvement.\n",
    "        patience=10,         # Number of epochs with no improvement to stop training.\n",
    "        verbose=2,          # Progress bar?\n",
    "        mode='auto'         # Direction of improvement is inferred.\n",
    "    )\n",
    "\n",
    "    # Start time\n",
    "    t_start = time.time()\n",
    "\n",
    "    # Train model\n",
    "    model_info = model.fit(\n",
    "        x=x_train,             # Training data\n",
    "        y=y_train,             # Label data\n",
    "        batch_size=batch_size,         # Number of samples per gradient update\n",
    "        epochs=num_epochs,             # Number of iterations over the entire dataset\n",
    "        verbose=2,             # Show progress bar\n",
    "        callbacks=[earlystop], # List of callbacks to apply during training\n",
    "        validation_split=0.2   # Fraction of the training data to be used as validation data\n",
    "    )\n",
    "\n",
    "    # End time\n",
    "    t_end = time.time()\n",
    "\n",
    "    loss = [float(x) for x in model_info.history['loss']]\n",
    "    val_loss = [float(x) for x in model_info.history['val_loss']]\n",
    "    training_time = t_end - t_start\n",
    "    return loss, val_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lstm_units in [50, 100, 500, 1000, 1500, 2000]:\n",
    "    loss, val_loss, training_time  = train_model(define_model(lstm_units), x_train, y_train)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-lstm_units_\" + str(lstm_units) + \"/loss.npy\", loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-lstm_units_\" + str(lstm_units) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-lstm_units_\" + str(lstm_units) + \"/training_time.npy\", training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gru_units in [2000]:#[50, 100, 500, 1000, 1500, 2000]:\n",
    "    loss, val_loss, training_time  = train_model(define_gru_model(gru_units), x_train, y_train)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-gru_units_\" + str(gru_units) + \"/loss.npy\", loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-gru_units_\" + str(gru_units) + \"/val_loss.npy\", val_loss)\n",
    "    write_hdfs_file(root_path + \"hyperparameter-tuning/all_sensors-gru_units_\" + str(gru_units) + \"/training_time.npy\", training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "val_losses = []\n",
    "training_times = []\n",
    "labels = []\n",
    "\n",
    "lstm_path = root_path + \"hyperparameter-tuning/all_sensors-lstm_units_\" \n",
    "gru_path = root_path + \"hyperparameter-tuning/all_sensors-gru_units_\" \n",
    "\n",
    "for lstm_units in [50, 100, 500, 1000, 1500, 2000]:\n",
    "    losses.append(read_hdfs_file(lstm_path + str(lstm_units) + \"/loss.npy\"))\n",
    "    val_losses.append(read_hdfs_file(lstm_path + str(lstm_units) + \"/val_loss.npy\"))\n",
    "    training_times.append(read_hdfs_file(lstm_path + str(lstm_units) + \"/training_time.npy\"))\n",
    "    labels.append(str(lstm_units) + \" lstm units\")\n",
    "\n",
    "for gru_units in [50, 100, 500, 1000, 1500, 2000]:\n",
    "    losses.append(read_hdfs_file(gru_path + str(gru_units) + \"/loss.npy\"))\n",
    "    val_losses.append(read_hdfs_file(gru_path + str(gru_units) + \"/val_loss.npy\"))\n",
    "    training_times.append(read_hdfs_file(gru_path + str(gru_units) + \"/training_time.npy\"))\n",
    "    labels.append(str(gru_units) + \" gru units\")\n",
    "    \n",
    "max_length = 0\n",
    "for loss in losses:\n",
    "    if len(loss) > max_length:\n",
    "        max_length = len(loss)\n",
    "for i in range(len(losses)):\n",
    "    for p in range(max_length - len(losses[i])):\n",
    "        losses[i] = np.append(losses[i], None)\n",
    "        val_losses[i] = np.append(val_losses[i], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o lstm_loss_df\n",
    "\n",
    "lstm_loss_df = spark.createDataFrame(\n",
    "    sc.parallelize(np.array(losses)[:6].transpose().tolist()), \n",
    "    np.array(labels)[:6].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.rc('font', **{'weight' : 'normal','size'   : 35})\n",
    "lstm_loss_plot = lstm_loss_df.plot.line(figsize=(40, 22), fontsize=40, linewidth=10)\n",
    " \n",
    "lstm_loss_plot.set_xlabel(\"Epoch\", {'size':50})\n",
    "lstm_loss_plot.set_ylabel(\"RMSE\", {'size':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o gru_loss_df\n",
    "\n",
    "gru_loss_df = spark.createDataFrame(\n",
    "    sc.parallelize(np.array(losses)[6:].transpose().tolist()), \n",
    "    np.array(labels)[6:].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.rc('font', **{'weight' : 'normal','size'   : 35})\n",
    "gru_loss_plot = gru_loss_df.plot.line(figsize=(40, 22), fontsize=40, linewidth=10)\n",
    " \n",
    "gru_loss_plot.set_xlabel(\"Epoch\", {'size':50})\n",
    "gru_loss_plot.set_ylabel(\"RMSE\", {'size':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o lstm_val_loss_df\n",
    "\n",
    "lstm_val_loss_df = spark.createDataFrame(\n",
    "    sc.parallelize(np.array(val_losses)[:6].transpose().tolist()), \n",
    "    np.array(labels)[:6].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.rc('font', **{'weight' : 'normal','size'   : 35})\n",
    "lstm_val_loss_plot = lstm_val_loss_df.plot.line(figsize=(40, 22), fontsize=40, linewidth=10)\n",
    " \n",
    "lstm_val_loss_plot.set_xlabel(\"Epoch\", {'size':50})\n",
    "lstm_val_loss_plot.set_ylabel(\"RMSE\", {'size':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%spark -o gru_val_loss_df\n",
    "\n",
    "gru_val_loss_df = spark.createDataFrame(\n",
    "    sc.parallelize(np.array(val_losses)[6:].transpose().tolist()), \n",
    "    np.array(labels)[6:].tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%local\n",
    "plt.rc('font', **{'weight' : 'normal','size'   : 35})\n",
    "gru_val_loss_plot = gru_val_loss_df.plot.line(figsize=(40, 22), fontsize=40, linewidth=10)\n",
    " \n",
    "gru_val_loss_plot.set_xlabel(\"Epoch\", {'size':50})\n",
    "gru_val_loss_plot.set_ylabel(\"RMSE\", {'size':50})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

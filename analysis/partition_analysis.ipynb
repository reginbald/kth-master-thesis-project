{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>44</td><td>application_1541669638743_0147</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop33:8088/proxy/application_1541669638743_0147/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop14:8042/node/containerlogs/container_e65_1541669638743_0147_01_000001/traffic_reginbald__jriv0000\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row, DataFrame\n",
    "\n",
    "from hops import hdfs\n",
    "from tempfile import TemporaryFile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/backward_partitions-3min.csv\"\n",
    "part5_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/backward_partitions-5min.csv\"\n",
    "part10_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/backward_partitions-10min.csv\"\n",
    "part20_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/backward_partitions-20min.csv\"\n",
    "part30_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/backward_partitions-30min.csv\"\n",
    "\n",
    "\n",
    "overlap_path = \"hdfs:///Projects/traffic_reginbald/processed_traffic_data/partitions/overlapping_partitions-base_weight_2_min-forward_3-backward_10.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_schema = StructType().add('node', StringType(), False) \\\n",
    "        .add('partition', ShortType(), False) \\\n",
    "        .add('group', StringType(), False) \n",
    "        \n",
    "overlap_schema = StructType().add('node', StringType(), False) \\\n",
    "        .add('partition', ShortType(), False) \\\n",
    "        .add('type', StringType(), False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "part3_df = spark.read.csv(part3_path, sep=';', schema=partition_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))\n",
    "part5_df  = spark.read.csv(part5_path, sep=';', schema=partition_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))\n",
    "part10_df = spark.read.csv(part10_path, sep=';', schema=partition_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))\n",
    "part20_df = spark.read.csv(part20_path, sep=';', schema=partition_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))\n",
    "part30_df = spark.read.csv(part30_path, sep=';', schema=partition_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))\n",
    "overlap_df = spark.read.csv(overlap_path, sep=';', schema=overlap_schema, ignoreLeadingWhiteSpace=True, \\\n",
    "                    ignoreTrailingWhiteSpace=True, timestampFormat='yyyy-MM-dd HH:mm:ss.SSS').where(~isnull(\"partition\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(partition)=35)\n",
      "Row(max(partition)=18)\n",
      "Row(max(partition)=12)\n",
      "Row(max(partition)=6)\n",
      "Row(max(partition)=6)"
     ]
    }
   ],
   "source": [
    "print part3_df.agg({\"partition\": \"max\"}).collect()[0]\n",
    "print part5_df.agg({\"partition\": \"max\"}).collect()[0]\n",
    "print part10_df.agg({\"partition\": \"max\"}).collect()[0]\n",
    "print part20_df.agg({\"partition\": \"max\"}).collect()[0]\n",
    "print part30_df.agg({\"partition\": \"max\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(avg(count)=23.805555555555557)\n",
      "Row(avg(count)=45.10526315789474)\n",
      "Row(avg(count)=65.92307692307692)\n",
      "Row(avg(count)=122.42857142857143)\n",
      "Row(avg(count)=122.42857142857143)"
     ]
    }
   ],
   "source": [
    "print part3_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]\n",
    "print part5_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]\n",
    "print part10_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]\n",
    "print part20_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]\n",
    "print part30_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(count)=194)\n",
      "Row(max(count)=245)\n",
      "Row(max(count)=408)\n",
      "Row(max(count)=678)\n",
      "Row(max(count)=678)"
     ]
    }
   ],
   "source": [
    "print part3_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]\n",
    "print part5_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]\n",
    "print part10_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]\n",
    "print part20_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]\n",
    "print part30_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(min(count)=1)\n",
      "Row(min(count)=1)\n",
      "Row(min(count)=1)\n",
      "Row(min(count)=11)\n",
      "Row(min(count)=11)"
     ]
    }
   ],
   "source": [
    "print part3_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]\n",
    "print part5_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]\n",
    "print part10_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]\n",
    "print part20_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]\n",
    "print part30_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(max(partition)=61)\n",
      "Row(avg(count)=268.9032258064516)\n",
      "Row(max(count)=609)\n",
      "Row(min(count)=11)"
     ]
    }
   ],
   "source": [
    "print overlap_df.agg({\"partition\": \"max\"}).collect()[0]\n",
    "print overlap_df.groupBy(\"partition\").count().agg({\"count\": \"avg\"}).collect()[0]\n",
    "print overlap_df.groupBy(\"partition\").count().agg({\"count\": \"max\"}).collect()[0]\n",
    "print overlap_df.groupBy(\"partition\").count().agg({\"count\": \"min\"}).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(node=u'E6N-12920', count=1)"
     ]
    }
   ],
   "source": [
    "print part3_df.groupBy(\"node\").count().collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
